{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8eaaf237",
      "metadata": {},
      "source": [
        "# Parallel Deep Learning: CNN Training on CIFAR-10\n",
        "\n",
        "## Assignment: Parallelization of Deep Learning Models\n",
        "\n",
        "This notebook implements:\n",
        "1. **Serial Baseline**: A CNN trained on CIFAR-10 using standard PyTorch\n",
        "2. **Parallel Implementation**: Data-parallel training using DistributedDataParallel (DDP)\n",
        "3. **Performance Analysis**: Comparison of training time, speedup, and scalability\n",
        "\n",
        "---\n",
        "\n",
        "## Part 1: Setup and Serial Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8321f23c",
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python -1.-1.-1' requires the ipykernel package.\n",
            "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'e:/parallel-dnn/.venv/Scripts/python.exe -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# Cell 1: Imports and Configuration\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Configuration\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 20\n",
        "LEARNING_RATE = 0.01\n",
        "MOMENTUM = 0.9\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef121940",
      "metadata": {},
      "source": [
        "### 1.1 CNN Architecture\n",
        "\n",
        "We implement a simple but effective CNN for CIFAR-10 classification:\n",
        "\n",
        "**Architecture Overview:**\n",
        "```\n",
        "Input (3x32x32) \n",
        "    → Conv1(3→32, 3x3, pad=1) → BatchNorm → ReLU → MaxPool(2x2)\n",
        "    → Conv2(32→64, 3x3, pad=1) → BatchNorm → ReLU → MaxPool(2x2)  \n",
        "    → Conv3(64→128, 3x3, pad=1) → BatchNorm → ReLU → MaxPool(2x2)\n",
        "    → Flatten → FC1(2048→512) → ReLU → Dropout(0.5)\n",
        "    → FC2(512→10) → Output (10 classes)\n",
        "```\n",
        "\n",
        "**Key Design Choices:**\n",
        "- BatchNorm after each conv layer for faster convergence\n",
        "- Dropout for regularization\n",
        "- Three conv layers provide sufficient feature extraction for 32x32 images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14578997",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 2: CNN Model Definition\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "    \"\"\"\n",
        "    A simple CNN architecture for CIFAR-10 classification.\n",
        "    \n",
        "    Architecture:\n",
        "    - 3 Convolutional blocks (Conv + BatchNorm + ReLU + MaxPool)\n",
        "    - 2 Fully connected layers with dropout\n",
        "    \n",
        "    Input: 3x32x32 (CIFAR-10 images)\n",
        "    Output: 10 classes\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, num_classes=10):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        \n",
        "        # Convolutional Block 1: 3 -> 32 channels\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        \n",
        "        # Convolutional Block 2: 32 -> 64 channels\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        \n",
        "        # Convolutional Block 3: 64 -> 128 channels\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        \n",
        "        # Pooling layer (used after each conv block)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        \n",
        "        # Fully connected layers\n",
        "        # After 3 pooling layers: 32 -> 16 -> 8 -> 4\n",
        "        # So feature map size is 128 * 4 * 4 = 2048\n",
        "        self.fc1 = nn.Linear(128 * 4 * 4, 512)\n",
        "        self.fc2 = nn.Linear(512, num_classes)\n",
        "        \n",
        "        # Dropout for regularization\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the network.\n",
        "        \n",
        "        Args:\n",
        "            x: Input tensor of shape (batch_size, 3, 32, 32)\n",
        "            \n",
        "        Returns:\n",
        "            Output tensor of shape (batch_size, num_classes)\n",
        "        \"\"\"\n",
        "        # Conv Block 1: (B, 3, 32, 32) -> (B, 32, 16, 16)\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
        "        \n",
        "        # Conv Block 2: (B, 32, 16, 16) -> (B, 64, 8, 8)\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
        "        \n",
        "        # Conv Block 3: (B, 64, 8, 8) -> (B, 128, 4, 4)\n",
        "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
        "        \n",
        "        # Flatten: (B, 128, 4, 4) -> (B, 2048)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        \n",
        "        # Fully connected layers\n",
        "        x = self.dropout(F.relu(self.fc1(x)))\n",
        "        x = self.fc2(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "# Create model and display architecture\n",
        "model = SimpleCNN().to(device)\n",
        "print(\"SimpleCNN Architecture:\")\n",
        "print(model)\n",
        "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f470d0fc",
      "metadata": {},
      "source": [
        "### 1.2 Data Loading and Preprocessing\n",
        "\n",
        "**CIFAR-10 Dataset:**\n",
        "- 60,000 32x32 color images in 10 classes\n",
        "- 50,000 training images, 10,000 test images\n",
        "- Classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck\n",
        "\n",
        "**Preprocessing:**\n",
        "- Normalization using CIFAR-10 statistics (mean and std per channel)\n",
        "- Data augmentation for training: Random horizontal flip and random crop with padding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f8d9e31",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 3: Data Loading and Preprocessing\n",
        "\n",
        "# CIFAR-10 normalization values (pre-computed statistics)\n",
        "CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)\n",
        "CIFAR10_STD = (0.2470, 0.2435, 0.2616)\n",
        "\n",
        "# Training transforms with data augmentation\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),           # Randomly flip horizontally\n",
        "    transforms.RandomCrop(32, padding=4),        # Random crop with padding\n",
        "    transforms.ToTensor(),                       # Convert to tensor [0, 1]\n",
        "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)  # Normalize\n",
        "])\n",
        "\n",
        "# Test transforms (no augmentation)\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
        "])\n",
        "\n",
        "# Download and load datasets\n",
        "DATA_DIR = './data'\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(\n",
        "    root=DATA_DIR, \n",
        "    train=True, \n",
        "    download=True, \n",
        "    transform=train_transform\n",
        ")\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(\n",
        "    root=DATA_DIR, \n",
        "    train=False, \n",
        "    download=True, \n",
        "    transform=test_transform\n",
        ")\n",
        "\n",
        "# Create data loaders for serial training\n",
        "train_loader = DataLoader(\n",
        "    train_dataset, \n",
        "    batch_size=BATCH_SIZE, \n",
        "    shuffle=True, \n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset, \n",
        "    batch_size=BATCH_SIZE, \n",
        "    shuffle=False, \n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "# Class names for CIFAR-10\n",
        "classes = ('airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "           'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "print(f\"Training set size: {len(train_dataset):,}\")\n",
        "print(f\"Test set size: {len(test_dataset):,}\")\n",
        "print(f\"Number of classes: {len(classes)}\")\n",
        "print(f\"Batch size: {BATCH_SIZE}\")\n",
        "print(f\"Training batches per epoch: {len(train_loader)}\")\n",
        "print(f\"Test batches: {len(test_loader)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bd046fd",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 3b: Visualize sample images from CIFAR-10\n",
        "\n",
        "def imshow(img, title=None):\n",
        "    \"\"\"Display a normalized image\"\"\"\n",
        "    # Unnormalize\n",
        "    img = img.cpu().numpy()\n",
        "    mean = np.array(CIFAR10_MEAN)\n",
        "    std = np.array(CIFAR10_STD)\n",
        "    img = std.reshape(3, 1, 1) * img + mean.reshape(3, 1, 1)\n",
        "    img = np.clip(img, 0, 1)\n",
        "    \n",
        "    plt.imshow(np.transpose(img, (1, 2, 0)))\n",
        "    if title:\n",
        "        plt.title(title)\n",
        "    plt.axis('off')\n",
        "\n",
        "# Get a batch of training images\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# Show 8 sample images\n",
        "fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
        "for idx, ax in enumerate(axes.flat):\n",
        "    img = images[idx]\n",
        "    label = classes[labels[idx]]\n",
        "    \n",
        "    # Unnormalize for display\n",
        "    img_display = img.numpy()\n",
        "    mean = np.array(CIFAR10_MEAN)\n",
        "    std = np.array(CIFAR10_STD)\n",
        "    img_display = std.reshape(3, 1, 1) * img_display + mean.reshape(3, 1, 1)\n",
        "    img_display = np.clip(img_display, 0, 1)\n",
        "    \n",
        "    ax.imshow(np.transpose(img_display, (1, 2, 0)))\n",
        "    ax.set_title(label)\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.suptitle('Sample CIFAR-10 Training Images', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d871b0f5",
      "metadata": {},
      "source": [
        "### 1.3 Serial Training Implementation\n",
        "\n",
        "The serial training loop follows the standard deep learning training process:\n",
        "\n",
        "1. **Forward Pass**: Input → Model → Predictions\n",
        "2. **Loss Computation**: Compare predictions with ground truth\n",
        "3. **Backward Pass**: Compute gradients via backpropagation  \n",
        "4. **Parameter Update**: Optimizer updates weights using gradients\n",
        "\n",
        "We implement timing instrumentation to measure baseline performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45bb4920",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 4: Serial Training Implementation\n",
        "\n",
        "class TrainingMetrics:\n",
        "    \"\"\"Class to store and track training metrics\"\"\"\n",
        "    def __init__(self):\n",
        "        self.train_losses = []\n",
        "        self.train_accuracies = []\n",
        "        self.test_losses = []\n",
        "        self.test_accuracies = []\n",
        "        self.epoch_times = []\n",
        "        self.total_time = 0\n",
        "        \n",
        "    def log_epoch(self, train_loss, train_acc, test_loss, test_acc, epoch_time):\n",
        "        self.train_losses.append(train_loss)\n",
        "        self.train_accuracies.append(train_acc)\n",
        "        self.test_losses.append(test_loss)\n",
        "        self.test_accuracies.append(test_acc)\n",
        "        self.epoch_times.append(epoch_time)\n",
        "\n",
        "\n",
        "def train_one_epoch(model, train_loader, optimizer, criterion, device):\n",
        "    \"\"\"\n",
        "    Train the model for one epoch.\n",
        "    \n",
        "    Args:\n",
        "        model: The neural network model\n",
        "        train_loader: DataLoader for training data\n",
        "        optimizer: The optimizer (e.g., SGD, Adam)\n",
        "        criterion: Loss function\n",
        "        device: Device to run on (CPU/GPU)\n",
        "        \n",
        "    Returns:\n",
        "        tuple: (average_loss, accuracy)\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        # Move data to device\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        \n",
        "        # 1. Zero gradients from previous iteration\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # 2. Forward pass\n",
        "        outputs = model(inputs)\n",
        "        \n",
        "        # 3. Compute loss\n",
        "        loss = criterion(outputs, targets)\n",
        "        \n",
        "        # 4. Backward pass (compute gradients)\n",
        "        loss.backward()\n",
        "        \n",
        "        # 5. Update parameters\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Track metrics\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "    \n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "    accuracy = 100. * correct / total\n",
        "    \n",
        "    return avg_loss, accuracy\n",
        "\n",
        "\n",
        "def evaluate(model, test_loader, criterion, device):\n",
        "    \"\"\"\n",
        "    Evaluate the model on test data.\n",
        "    \n",
        "    Args:\n",
        "        model: The neural network model\n",
        "        test_loader: DataLoader for test data\n",
        "        criterion: Loss function\n",
        "        device: Device to run on\n",
        "        \n",
        "    Returns:\n",
        "        tuple: (average_loss, accuracy)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            \n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            \n",
        "            running_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "    \n",
        "    avg_loss = running_loss / len(test_loader)\n",
        "    accuracy = 100. * correct / total\n",
        "    \n",
        "    return avg_loss, accuracy\n",
        "\n",
        "\n",
        "def train_serial(model, train_loader, test_loader, optimizer, criterion, \n",
        "                 epochs, device, verbose=True):\n",
        "    \"\"\"\n",
        "    Complete serial training loop with metrics tracking.\n",
        "    \n",
        "    Args:\n",
        "        model: The neural network model\n",
        "        train_loader: DataLoader for training data\n",
        "        test_loader: DataLoader for test data\n",
        "        optimizer: The optimizer\n",
        "        criterion: Loss function\n",
        "        epochs: Number of training epochs\n",
        "        device: Device to run on\n",
        "        verbose: Whether to print progress\n",
        "        \n",
        "    Returns:\n",
        "        TrainingMetrics: Object containing all training metrics\n",
        "    \"\"\"\n",
        "    metrics = TrainingMetrics()\n",
        "    total_start_time = time.perf_counter()\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        epoch_start_time = time.perf_counter()\n",
        "        \n",
        "        # Train for one epoch\n",
        "        train_loss, train_acc = train_one_epoch(\n",
        "            model, train_loader, optimizer, criterion, device\n",
        "        )\n",
        "        \n",
        "        # Evaluate on test set\n",
        "        test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
        "        \n",
        "        epoch_time = time.perf_counter() - epoch_start_time\n",
        "        \n",
        "        # Log metrics\n",
        "        metrics.log_epoch(train_loss, train_acc, test_loss, test_acc, epoch_time)\n",
        "        \n",
        "        if verbose:\n",
        "            print(f\"Epoch [{epoch+1:2d}/{epochs}] | \"\n",
        "                  f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | \"\n",
        "                  f\"Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.2f}% | \"\n",
        "                  f\"Time: {epoch_time:.2f}s\")\n",
        "    \n",
        "    metrics.total_time = time.perf_counter() - total_start_time\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Training completed in {metrics.total_time:.2f} seconds\")\n",
        "        print(f\"Average epoch time: {np.mean(metrics.epoch_times):.2f}s\")\n",
        "        print(f\"Final Test Accuracy: {metrics.test_accuracies[-1]:.2f}%\")\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "print(\"Training functions defined successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb31b8eb",
      "metadata": {},
      "source": [
        "### 1.4 Run Serial Baseline Experiment\n",
        "\n",
        "Now we train the model using the serial implementation to establish our baseline performance metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d90a9b7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 5: Run Serial Baseline Training\n",
        "\n",
        "# Reinitialize model for fresh training\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "model_serial = SimpleCNN().to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(\n",
        "    model_serial.parameters(), \n",
        "    lr=LEARNING_RATE, \n",
        "    momentum=MOMENTUM,\n",
        "    weight_decay=1e-4  # L2 regularization\n",
        ")\n",
        "\n",
        "# Optional: Learning rate scheduler\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "print(\"Starting Serial Training...\")\n",
        "print(f\"Configuration: Epochs={EPOCHS}, Batch Size={BATCH_SIZE}, LR={LEARNING_RATE}\")\n",
        "print(f\"Device: {device}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Run training\n",
        "serial_metrics = train_serial(\n",
        "    model=model_serial,\n",
        "    train_loader=train_loader,\n",
        "    test_loader=test_loader,\n",
        "    optimizer=optimizer,\n",
        "    criterion=criterion,\n",
        "    epochs=EPOCHS,\n",
        "    device=device,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Store baseline time for comparison\n",
        "serial_total_time = serial_metrics.total_time\n",
        "print(f\"\\nSerial baseline total time: {serial_total_time:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afb96071",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 5b: Visualize Serial Training Results\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "# Plot 1: Loss curves\n",
        "axes[0].plot(serial_metrics.train_losses, 'b-', label='Train Loss', linewidth=2)\n",
        "axes[0].plot(serial_metrics.test_losses, 'r-', label='Test Loss', linewidth=2)\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].set_title('Training and Test Loss')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Accuracy curves\n",
        "axes[1].plot(serial_metrics.train_accuracies, 'b-', label='Train Acc', linewidth=2)\n",
        "axes[1].plot(serial_metrics.test_accuracies, 'r-', label='Test Acc', linewidth=2)\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Accuracy (%)')\n",
        "axes[1].set_title('Training and Test Accuracy')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 3: Epoch times\n",
        "axes[2].bar(range(1, EPOCHS+1), serial_metrics.epoch_times, color='green', alpha=0.7)\n",
        "axes[2].axhline(y=np.mean(serial_metrics.epoch_times), color='red', \n",
        "                linestyle='--', label=f'Avg: {np.mean(serial_metrics.epoch_times):.2f}s')\n",
        "axes[2].set_xlabel('Epoch')\n",
        "axes[2].set_ylabel('Time (seconds)')\n",
        "axes[2].set_title('Time per Epoch')\n",
        "axes[2].legend()\n",
        "axes[2].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.suptitle('Serial Training Results', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print summary\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SERIAL BASELINE SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Total Training Time:     {serial_metrics.total_time:.2f} seconds\")\n",
        "print(f\"Average Epoch Time:      {np.mean(serial_metrics.epoch_times):.2f} seconds\")\n",
        "print(f\"Final Training Accuracy: {serial_metrics.train_accuracies[-1]:.2f}%\")\n",
        "print(f\"Final Test Accuracy:     {serial_metrics.test_accuracies[-1]:.2f}%\")\n",
        "print(f\"Best Test Accuracy:      {max(serial_metrics.test_accuracies):.2f}%\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07ba484a",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 2: Parallel Training Concepts (Theory)\n",
        "\n",
        "Before implementing parallel training, we need to understand the key concepts that make data parallelism work."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4cfab283",
      "metadata": {},
      "source": [
        "### 2.1 What is Data Parallelism?\n",
        "\n",
        "**Data Parallelism** is a parallelization strategy where:\n",
        "1. The **same model** is replicated across multiple workers (processes/GPUs)\n",
        "2. The **training data is partitioned** so each worker processes different samples\n",
        "3. **Gradients are synchronized** across workers after each backward pass\n",
        "4. All workers **update their model identically**, keeping them in sync\n",
        "\n",
        "```\n",
        "┌─────────────────────────────────────────────────────────────────────────┐\n",
        "│                         DATA PARALLEL TRAINING                          │\n",
        "├─────────────────────────────────────────────────────────────────────────┤\n",
        "│                                                                         │\n",
        "│   Full Dataset: [████████████████████████████████████████████████]      │\n",
        "│                          50,000 samples                                  │\n",
        "│                               │                                          │\n",
        "│           ┌───────────┬───────┴───────┬───────────┐                     │\n",
        "│           ▼           ▼               ▼           ▼                     │\n",
        "│   ┌─────────────┬─────────────┬─────────────┬─────────────┐            │\n",
        "│   │  Worker 0   │  Worker 1   │  Worker 2   │  Worker 3   │            │\n",
        "│   │  (Rank 0)   │  (Rank 1)   │  (Rank 2)   │  (Rank 3)   │            │\n",
        "│   ├─────────────┼─────────────┼─────────────┼─────────────┤            │\n",
        "│   │ Model Copy  │ Model Copy  │ Model Copy  │ Model Copy  │            │\n",
        "│   │ [████████]  │ [████████]  │ [████████]  │ [████████]  │            │\n",
        "│   ├─────────────┼─────────────┼─────────────┼─────────────┤            │\n",
        "│   │ Data Shard  │ Data Shard  │ Data Shard  │ Data Shard  │            │\n",
        "│   │ 0 - 12,499  │12,500-24,999│25,000-37,499│37,500-49,999│            │\n",
        "│   └─────────────┴─────────────┴─────────────┴─────────────┘            │\n",
        "│                                                                         │\n",
        "│   Each worker:                                                          │\n",
        "│   1. Forward pass on LOCAL batch                                        │\n",
        "│   2. Backward pass → compute LOCAL gradients                            │\n",
        "│   3. AllReduce → AVERAGE gradients across all workers                   │\n",
        "│   4. Update parameters → all workers stay SYNCHRONIZED                  │\n",
        "│                                                                         │\n",
        "└─────────────────────────────────────────────────────────────────────────┘\n",
        "```\n",
        "\n",
        "**Why Data Parallelism Works Mathematically:**\n",
        "\n",
        "For gradient descent with batch size $B$ across $N$ workers:\n",
        "\n",
        "$$\\nabla L_{total} = \\frac{1}{N} \\sum_{i=1}^{N} \\nabla L_i$$\n",
        "\n",
        "Where $\\nabla L_i$ is the gradient computed by worker $i$ on its local batch. By averaging gradients across all workers, we effectively compute the gradient over the combined batch of size $B \\times N$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5051208",
      "metadata": {},
      "source": [
        "### 2.2 AllReduce: The Core of Gradient Synchronization\n",
        "\n",
        "**AllReduce** is a collective communication operation that:\n",
        "1. **Collects** values from all workers\n",
        "2. **Applies a reduction** operation (SUM, AVG, MAX, etc.)\n",
        "3. **Distributes** the result back to ALL workers\n",
        "\n",
        "This is different from:\n",
        "- **Reduce**: Sends result to only ONE worker (the root)\n",
        "- **Broadcast**: Sends from one worker to all others\n",
        "- **AllGather**: Collects values without reduction\n",
        "\n",
        "```\n",
        "┌─────────────────────────────────────────────────────────────────────────┐\n",
        "│                         AllReduce Operation                             │\n",
        "├─────────────────────────────────────────────────────────────────────────┤\n",
        "│                                                                         │\n",
        "│   BEFORE AllReduce (each worker has different local gradients):         │\n",
        "│                                                                         │\n",
        "│   ┌─────────────┐ ┌─────────────┐ ┌─────────────┐ ┌─────────────┐      │\n",
        "│   │  Worker 0   │ │  Worker 1   │ │  Worker 2   │ │  Worker 3   │      │\n",
        "│   │   grad = 2  │ │   grad = 4  │ │   grad = 6  │ │   grad = 8  │      │\n",
        "│   └──────┬──────┘ └──────┬──────┘ └──────┬──────┘ └──────┬──────┘      │\n",
        "│          │               │               │               │              │\n",
        "│          └───────────────┴───────┬───────┴───────────────┘              │\n",
        "│                                  │                                       │\n",
        "│                                  ▼                                       │\n",
        "│                    ┌─────────────────────────────┐                      │\n",
        "│                    │   REDUCTION OPERATION        │                      │\n",
        "│                    │   SUM = 2 + 4 + 6 + 8 = 20  │                      │\n",
        "│                    │   AVG = 20 / 4 = 5          │                      │\n",
        "│                    └─────────────┬───────────────┘                      │\n",
        "│                                  │                                       │\n",
        "│          ┌───────────────┬───────┴───────┬───────────────┐              │\n",
        "│          │               │               │               │              │\n",
        "│          ▼               ▼               ▼               ▼              │\n",
        "│   ┌─────────────┐ ┌─────────────┐ ┌─────────────┐ ┌─────────────┐      │\n",
        "│   │  Worker 0   │ │  Worker 1   │ │  Worker 2   │ │  Worker 3   │      │\n",
        "│   │   grad = 5  │ │   grad = 5  │ │   grad = 5  │ │   grad = 5  │      │\n",
        "│   └─────────────┘ └─────────────┘ └─────────────┘ └─────────────┘      │\n",
        "│                                                                         │\n",
        "│   AFTER AllReduce: All workers have IDENTICAL averaged gradients        │\n",
        "│                                                                         │\n",
        "└─────────────────────────────────────────────────────────────────────────┘\n",
        "```\n",
        "\n",
        "**Key Insight**: After AllReduce, every worker has exactly the same gradient values. This means when each worker applies `optimizer.step()`, they all make the identical parameter update, keeping all model copies synchronized!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6eacf57b",
      "metadata": {},
      "source": [
        "### 2.3 Ring-AllReduce Algorithm\n",
        "\n",
        "The naive AllReduce approach (all-to-one-to-all) has a communication bottleneck at the root. Modern implementations use **Ring-AllReduce**, which is bandwidth-optimal.\n",
        "\n",
        "**How Ring-AllReduce Works:**\n",
        "\n",
        "Each worker is arranged in a logical ring. The algorithm has two phases:\n",
        "\n",
        "```\n",
        "┌─────────────────────────────────────────────────────────────────────────┐\n",
        "│                      Ring-AllReduce Algorithm                           │\n",
        "├─────────────────────────────────────────────────────────────────────────┤\n",
        "│                                                                         │\n",
        "│   SETUP: 4 workers in a ring, each with gradient split into 4 chunks   │\n",
        "│                                                                         │\n",
        "│   Worker 0: [A0][B0][C0][D0]    Worker 1: [A1][B1][C1][D1]              │\n",
        "│   Worker 2: [A2][B2][C2][D2]    Worker 3: [A3][B3][C3][D3]              │\n",
        "│                                                                         │\n",
        "├─────────────────────────────────────────────────────────────────────────┤\n",
        "│   PHASE 1: Scatter-Reduce (N-1 steps)                                   │\n",
        "│   Each worker sends one chunk to the next, receives and accumulates     │\n",
        "│                                                                         │\n",
        "│        ┌──────┐        ┌──────┐        ┌──────┐        ┌──────┐        │\n",
        "│        │  W0  │───────►│  W1  │───────►│  W2  │───────►│  W3  │        │\n",
        "│        └──────┘        └──────┘        └──────┘        └──────┘        │\n",
        "│            ▲                                               │            │\n",
        "│            └───────────────────────────────────────────────┘            │\n",
        "│                                                                         │\n",
        "│   After Phase 1: Each worker has the COMPLETE SUM of one chunk         │\n",
        "│   W0 has: Σ(A)    W1 has: Σ(B)    W2 has: Σ(C)    W3 has: Σ(D)         │\n",
        "│                                                                         │\n",
        "├─────────────────────────────────────────────────────────────────────────┤\n",
        "│   PHASE 2: AllGather (N-1 steps)                                        │\n",
        "│   Each worker sends its complete chunk to the next worker               │\n",
        "│                                                                         │\n",
        "│        ┌──────┐        ┌──────┐        ┌──────┐        ┌──────┐        │\n",
        "│        │  W0  │───────►│  W1  │───────►│  W2  │───────►│  W3  │        │\n",
        "│        └──────┘        └──────┘        └──────┘        └──────┘        │\n",
        "│            ▲                                               │            │\n",
        "│            └───────────────────────────────────────────────┘            │\n",
        "│                                                                         │\n",
        "│   After Phase 2: Every worker has ALL reduced chunks                    │\n",
        "│   All workers have: [Σ(A)][Σ(B)][Σ(C)][Σ(D)]                           │\n",
        "│                                                                         │\n",
        "└─────────────────────────────────────────────────────────────────────────┘\n",
        "```\n",
        "\n",
        "**Complexity Analysis:**\n",
        "\n",
        "| Approach | Communication Volume | Bandwidth Efficiency |\n",
        "|----------|---------------------|---------------------|\n",
        "| Naive (all-to-one-to-all) | $O(N \\cdot M)$ at root | Poor (root bottleneck) |\n",
        "| Ring-AllReduce | $O(\\frac{2(N-1)}{N} \\cdot M)$ per worker | Optimal (distributed) |\n",
        "\n",
        "Where $N$ = number of workers, $M$ = message size (gradient size)\n",
        "\n",
        "**Why Ring-AllReduce is Efficient:**\n",
        "- No single bottleneck node\n",
        "- Each worker sends/receives exactly $\\frac{2(N-1)}{N} \\cdot M$ data\n",
        "- Approaches $2M$ for large $N$ (only 2x the gradient size, regardless of worker count!)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e3fb2df",
      "metadata": {},
      "source": [
        "### 2.4 DistributedDataParallel (DDP) Under the Hood\n",
        "\n",
        "PyTorch's `DistributedDataParallel` automates AllReduce with several optimizations:\n",
        "\n",
        "```\n",
        "┌─────────────────────────────────────────────────────────────────────────┐\n",
        "│           What DDP Does During loss.backward()                          │\n",
        "├─────────────────────────────────────────────────────────────────────────┤\n",
        "│                                                                         │\n",
        "│   1. PyTorch computes gradients layer by layer (from output to input)  │\n",
        "│                                                                         │\n",
        "│   2. DDP registers HOOKS on each parameter's gradient computation       │\n",
        "│                                                                         │\n",
        "│   3. DDP groups parameters into BUCKETS for efficient communication:    │\n",
        "│                                                                         │\n",
        "│      ┌─────────────────────────────────────────────────────────────┐   │\n",
        "│      │  Bucket 1         Bucket 2         Bucket 3         Bucket N │   │\n",
        "│      │ [fc2.weight]    [fc1.weight]    [conv3.weight]    [conv1...]  │   │\n",
        "│      │ [fc2.bias  ]    [fc1.bias  ]    [conv3.bias  ]    [        ]  │   │\n",
        "│      │     ↓                ↓                ↓                ↓      │   │\n",
        "│      │  AllReduce       AllReduce        AllReduce        AllReduce  │   │\n",
        "│      │    (async)         (async)          (async)          (async)  │   │\n",
        "│      └─────────────────────────────────────────────────────────────┘   │\n",
        "│                                                                         │\n",
        "│   4. When a bucket is FULL, DDP starts AllReduce ASYNCHRONOUSLY         │\n",
        "│      while backward continues computing other gradients                 │\n",
        "│                                                                         │\n",
        "│   5. OVERLAP of computation and communication:                          │\n",
        "│                                                                         │\n",
        "│      Time ──────────────────────────────────────────────────────►      │\n",
        "│                                                                         │\n",
        "│      Backward:  [fc2][fc1][conv3][conv2][conv1]                        │\n",
        "│      AllReduce:     [bucket1][bucket2][bucket3]...                     │\n",
        "│                        ↑                                                │\n",
        "│                   Starts as soon as bucket ready                       │\n",
        "│                                                                         │\n",
        "│   6. By the time backward() returns:                                    │\n",
        "│      - All AllReduce operations have completed                          │\n",
        "│      - All workers have identical averaged gradients                    │\n",
        "│                                                                         │\n",
        "└─────────────────────────────────────────────────────────────────────────┘\n",
        "```\n",
        "\n",
        "**DDP Optimizations Summary:**\n",
        "\n",
        "| Optimization | Benefit |\n",
        "|--------------|---------|\n",
        "| **Gradient Bucketing** | Reduces number of AllReduce calls; larger messages are more efficient |\n",
        "| **Overlapped Communication** | Hides communication latency behind computation |\n",
        "| **Lazy Initialization** | Buckets determined at first forward pass based on gradient order |\n",
        "| **Parameter Broadcast** | Ensures all workers start with identical parameters |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e63db3ab",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 3: Parallel Implementation\n",
        "\n",
        "Now we implement data-parallel training using PyTorch's DistributedDataParallel (DDP).\n",
        "\n",
        "### 3.1 Distributed Setup Utilities\n",
        "\n",
        "We need to initialize the process group that enables communication between workers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56c3f5f2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 9: Distributed Setup Utilities\n",
        "\n",
        "import torch.distributed as dist\n",
        "import torch.multiprocessing as mp\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "\n",
        "def setup_distributed(rank, world_size):\n",
        "    \"\"\"\n",
        "    Initialize the distributed process group.\n",
        "    \n",
        "    Args:\n",
        "        rank: Unique identifier for this process (0 to world_size-1)\n",
        "        world_size: Total number of processes\n",
        "        \n",
        "    The process group enables collective communication operations like AllReduce.\n",
        "    \"\"\"\n",
        "    # Set environment variables for the rendezvous\n",
        "    os.environ['MASTER_ADDR'] = 'localhost'\n",
        "    os.environ['MASTER_PORT'] = '12355'\n",
        "    \n",
        "    # Initialize the process group\n",
        "    # Backend options:\n",
        "    #   - 'gloo': Works on CPU and GPU, supports Windows\n",
        "    #   - 'nccl': Optimized for NVIDIA GPUs, Linux only\n",
        "    #   - 'mpi': Requires MPI installation\n",
        "    backend = 'gloo'  # Use 'nccl' for GPU training on Linux\n",
        "    \n",
        "    dist.init_process_group(\n",
        "        backend=backend,\n",
        "        init_method='env://',\n",
        "        world_size=world_size,\n",
        "        rank=rank\n",
        "    )\n",
        "    \n",
        "    if rank == 0:\n",
        "        print(f\"Distributed process group initialized with {world_size} workers\")\n",
        "        print(f\"Backend: {backend}\")\n",
        "\n",
        "\n",
        "def cleanup_distributed():\n",
        "    \"\"\"\n",
        "    Clean up the distributed process group.\n",
        "    Call this at the end of training to properly release resources.\n",
        "    \"\"\"\n",
        "    dist.destroy_process_group()\n",
        "\n",
        "\n",
        "def get_rank():\n",
        "    \"\"\"Get the rank of current process.\"\"\"\n",
        "    if dist.is_initialized():\n",
        "        return dist.get_rank()\n",
        "    return 0\n",
        "\n",
        "\n",
        "def get_world_size():\n",
        "    \"\"\"Get the total number of processes.\"\"\"\n",
        "    if dist.is_initialized():\n",
        "        return dist.get_world_size()\n",
        "    return 1\n",
        "\n",
        "\n",
        "def is_main_process():\n",
        "    \"\"\"Check if this is the main process (rank 0).\"\"\"\n",
        "    return get_rank() == 0\n",
        "\n",
        "print(\"Distributed utilities defined successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cafeb9a8",
      "metadata": {},
      "source": [
        "### 3.2 Data Partitioning with DistributedSampler\n",
        "\n",
        "The `DistributedSampler` automatically partitions the dataset so each worker sees a different subset of the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9323a969",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 10: Distributed Data Loader with DistributedSampler\n",
        "\n",
        "def create_distributed_dataloaders(train_dataset, test_dataset, batch_size, \n",
        "                                    rank, world_size, num_workers=2):\n",
        "    \"\"\"\n",
        "    Create DataLoaders with DistributedSampler for parallel training.\n",
        "    \n",
        "    Args:\n",
        "        train_dataset: Training dataset\n",
        "        test_dataset: Test dataset  \n",
        "        batch_size: Batch size PER WORKER (effective batch = batch_size * world_size)\n",
        "        rank: Current worker's rank\n",
        "        world_size: Total number of workers\n",
        "        num_workers: Number of data loading workers\n",
        "        \n",
        "    Returns:\n",
        "        tuple: (train_loader, test_loader, train_sampler)\n",
        "        \n",
        "    Note: We return train_sampler so we can call set_epoch() for proper shuffling.\n",
        "    \"\"\"\n",
        "    # DistributedSampler partitions the dataset across workers\n",
        "    # Each worker will see len(dataset) / world_size samples\n",
        "    train_sampler = DistributedSampler(\n",
        "        train_dataset,\n",
        "        num_replicas=world_size,  # Total number of workers\n",
        "        rank=rank,                 # This worker's ID\n",
        "        shuffle=True,              # Shuffle within each worker's partition\n",
        "        drop_last=True             # Drop incomplete batches for consistent batch sizes\n",
        "    )\n",
        "    \n",
        "    test_sampler = DistributedSampler(\n",
        "        test_dataset,\n",
        "        num_replicas=world_size,\n",
        "        rank=rank,\n",
        "        shuffle=False,  # No shuffling for test data\n",
        "        drop_last=False\n",
        "    )\n",
        "    \n",
        "    # Create DataLoaders with the samplers\n",
        "    # Note: shuffle must be False when using a sampler\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        sampler=train_sampler,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True,\n",
        "        drop_last=True\n",
        "    )\n",
        "    \n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=batch_size,\n",
        "        sampler=test_sampler,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True\n",
        "    )\n",
        "    \n",
        "    return train_loader, test_loader, train_sampler\n",
        "\n",
        "# Explain effective batch size\n",
        "print(\"DistributedSampler Behavior:\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Total training samples: {len(train_dataset):,}\")\n",
        "print(f\"Batch size per worker: {BATCH_SIZE}\")\n",
        "print()\n",
        "for num_workers in [1, 2, 4]:\n",
        "    samples_per_worker = len(train_dataset) // num_workers\n",
        "    effective_batch = BATCH_SIZE * num_workers\n",
        "    batches_per_worker = samples_per_worker // BATCH_SIZE\n",
        "    print(f\"With {num_workers} workers:\")\n",
        "    print(f\"  - Samples per worker: {samples_per_worker:,}\")\n",
        "    print(f\"  - Effective batch size: {effective_batch}\")\n",
        "    print(f\"  - Batches per worker per epoch: {batches_per_worker}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f408e13c",
      "metadata": {},
      "source": [
        "### 3.3 Parallel Training Loop with DDP\n",
        "\n",
        "The parallel training loop wraps the model with DDP and uses the distributed sampler. DDP automatically handles gradient synchronization via AllReduce."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "544052c8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 11: Parallel Training Loop with DDP\n",
        "\n",
        "def train_one_epoch_parallel(model, train_loader, optimizer, criterion, device, rank):\n",
        "    \"\"\"\n",
        "    Train for one epoch in parallel mode.\n",
        "    \n",
        "    The model is wrapped with DDP, which automatically:\n",
        "    1. Computes local gradients on this worker's data\n",
        "    2. Calls AllReduce to average gradients across all workers\n",
        "    3. Ensures all workers have identical gradients after backward()\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        \n",
        "        # Zero gradients\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Forward pass (local computation)\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        \n",
        "        # Backward pass\n",
        "        # DDP automatically synchronizes gradients via AllReduce here!\n",
        "        # When loss.backward() completes:\n",
        "        #   - Each worker has computed its local gradients\n",
        "        #   - DDP has called AllReduce to average gradients across workers\n",
        "        #   - All workers now have identical averaged gradients\n",
        "        loss.backward()\n",
        "        \n",
        "        # Update parameters\n",
        "        # Since all workers have identical gradients, they make identical updates\n",
        "        # This keeps all model copies synchronized\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Track metrics (local to this worker)\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "    \n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "    accuracy = 100. * correct / total\n",
        "    \n",
        "    return avg_loss, accuracy\n",
        "\n",
        "\n",
        "def evaluate_parallel(model, test_loader, criterion, device):\n",
        "    \"\"\"Evaluate model (same as serial, but on distributed data).\"\"\"\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            \n",
        "            running_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "    \n",
        "    # Aggregate metrics across all workers\n",
        "    if dist.is_initialized():\n",
        "        # Sum correct and total across all workers\n",
        "        correct_tensor = torch.tensor([correct], device=device)\n",
        "        total_tensor = torch.tensor([total], device=device)\n",
        "        loss_tensor = torch.tensor([running_loss], device=device)\n",
        "        \n",
        "        dist.all_reduce(correct_tensor, op=dist.ReduceOp.SUM)\n",
        "        dist.all_reduce(total_tensor, op=dist.ReduceOp.SUM)\n",
        "        dist.all_reduce(loss_tensor, op=dist.ReduceOp.SUM)\n",
        "        \n",
        "        correct = correct_tensor.item()\n",
        "        total = total_tensor.item()\n",
        "        running_loss = loss_tensor.item()\n",
        "    \n",
        "    avg_loss = running_loss / len(test_loader) / (get_world_size() if dist.is_initialized() else 1)\n",
        "    accuracy = 100. * correct / total\n",
        "    \n",
        "    return avg_loss, accuracy\n",
        "\n",
        "\n",
        "def train_parallel_worker(rank, world_size, epochs, batch_size, lr, momentum,\n",
        "                          train_dataset, test_dataset, results_queue=None):\n",
        "    \"\"\"\n",
        "    Main training function for each parallel worker.\n",
        "    \n",
        "    This function is spawned by torch.multiprocessing for each worker.\n",
        "    \n",
        "    Args:\n",
        "        rank: This worker's unique ID (0 to world_size-1)\n",
        "        world_size: Total number of workers\n",
        "        epochs: Number of training epochs\n",
        "        batch_size: Batch size per worker\n",
        "        lr: Learning rate\n",
        "        momentum: SGD momentum\n",
        "        train_dataset: Training dataset (shared)\n",
        "        test_dataset: Test dataset (shared)\n",
        "        results_queue: Queue to return results to main process\n",
        "    \"\"\"\n",
        "    # 1. Initialize distributed process group\n",
        "    setup_distributed(rank, world_size)\n",
        "    \n",
        "    # 2. Set device (GPU if available)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    \n",
        "    # 3. Create model with same initialization on all workers\n",
        "    torch.manual_seed(RANDOM_SEED)  # Same seed = same initial weights\n",
        "    model = SimpleCNN().to(device)\n",
        "    \n",
        "    # 4. Wrap model with DDP for automatic gradient synchronization\n",
        "    # DDP registers hooks that call AllReduce during backward()\n",
        "    model = DDP(model)\n",
        "    \n",
        "    # 5. Create distributed data loaders\n",
        "    train_loader, test_loader, train_sampler = create_distributed_dataloaders(\n",
        "        train_dataset, test_dataset, batch_size, rank, world_size\n",
        "    )\n",
        "    \n",
        "    # 6. Setup optimizer and loss\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=1e-4)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    # 7. Training loop\n",
        "    metrics = TrainingMetrics()\n",
        "    total_start_time = time.perf_counter()\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        epoch_start_time = time.perf_counter()\n",
        "        \n",
        "        # IMPORTANT: Set epoch for proper shuffling across epochs\n",
        "        # This ensures different shuffling each epoch while maintaining\n",
        "        # deterministic behavior for reproducibility\n",
        "        train_sampler.set_epoch(epoch)\n",
        "        \n",
        "        # Train for one epoch\n",
        "        train_loss, train_acc = train_one_epoch_parallel(\n",
        "            model, train_loader, optimizer, criterion, device, rank\n",
        "        )\n",
        "        \n",
        "        # Evaluate\n",
        "        test_loss, test_acc = evaluate_parallel(model, test_loader, criterion, device)\n",
        "        \n",
        "        epoch_time = time.perf_counter() - epoch_start_time\n",
        "        metrics.log_epoch(train_loss, train_acc, test_loss, test_acc, epoch_time)\n",
        "        \n",
        "        # Only rank 0 prints to avoid duplicate output\n",
        "        if rank == 0:\n",
        "            print(f\"Epoch [{epoch+1:2d}/{epochs}] | \"\n",
        "                  f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | \"\n",
        "                  f\"Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.2f}% | \"\n",
        "                  f\"Time: {epoch_time:.2f}s\")\n",
        "    \n",
        "    metrics.total_time = time.perf_counter() - total_start_time\n",
        "    \n",
        "    # Return results from rank 0\n",
        "    if rank == 0 and results_queue is not None:\n",
        "        results_queue.put({\n",
        "            'train_losses': metrics.train_losses,\n",
        "            'train_accuracies': metrics.train_accuracies,\n",
        "            'test_losses': metrics.test_losses,\n",
        "            'test_accuracies': metrics.test_accuracies,\n",
        "            'epoch_times': metrics.epoch_times,\n",
        "            'total_time': metrics.total_time\n",
        "        })\n",
        "    \n",
        "    # Cleanup\n",
        "    cleanup_distributed()\n",
        "\n",
        "print(\"Parallel training functions defined successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3711921",
      "metadata": {},
      "source": [
        "### 3.4 Launcher Script for Multi-Process Execution\n",
        "\n",
        "Due to Python multiprocessing limitations (especially on Windows), we need to create a standalone script for parallel training. This script can be run with `torchrun` or `python -m torch.distributed.launch`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c70014d6",
      "metadata": {},
      "source": [
        "### 3.5 Run Parallel Training Experiments\n",
        "\n",
        "We'll run the parallel training script and collect results for different numbers of workers.\n",
        "\n",
        "**Note:** Due to Jupyter notebook limitations with multiprocessing on Windows, we run the parallel training via subprocess calling the standalone script."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "784ee3cc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Cell 13: Run Parallel Training Experiments\n",
        "\n",
        "# import subprocess\n",
        "# import json\n",
        "\n",
        "# def run_parallel_experiment(num_workers, epochs=EPOCHS, batch_size=BATCH_SIZE):\n",
        "#     \"\"\"\n",
        "#     Run parallel training with specified number of workers.\n",
        "    \n",
        "#     Args:\n",
        "#         num_workers: Number of parallel workers\n",
        "#         epochs: Number of training epochs\n",
        "#         batch_size: Batch size per worker\n",
        "        \n",
        "#     Returns:\n",
        "#         dict: Results from training\n",
        "#     \"\"\"\n",
        "#     print(f\"\\n{'='*60}\")\n",
        "#     print(f\"Running parallel training with {num_workers} workers...\")\n",
        "#     print(f\"{'='*60}\\n\")\n",
        "    \n",
        "#     # Run the parallel training script\n",
        "#     cmd = [\n",
        "#         'python', 'parallel_training.py',\n",
        "#         '--epochs', str(epochs),\n",
        "#         '--batch-size', str(batch_size),\n",
        "#         '--world-size', str(num_workers)\n",
        "#     ]\n",
        "    \n",
        "#     start_time = time.perf_counter()\n",
        "    \n",
        "#     try:\n",
        "#         result = subprocess.run(\n",
        "#             cmd,\n",
        "#             capture_output=True,\n",
        "#             text=True,\n",
        "#             timeout=600  # 10 minute timeout\n",
        "#         )\n",
        "        \n",
        "#         # Print output\n",
        "#         if result.stdout:\n",
        "#             print(result.stdout)\n",
        "#         if result.stderr:\n",
        "#             print(\"Errors:\", result.stderr)\n",
        "            \n",
        "#     except subprocess.TimeoutExpired:\n",
        "#         print(\"Training timed out!\")\n",
        "#         return None\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error running training: {e}\")\n",
        "#         return None\n",
        "    \n",
        "#     # Load results from saved file\n",
        "#     results_file = f'parallel_results_{num_workers}workers.json'\n",
        "#     try:\n",
        "#         with open(results_file, 'r') as f:\n",
        "#             results = json.load(f)\n",
        "#         return results\n",
        "#     except FileNotFoundError:\n",
        "#         print(f\"Results file not found: {results_file}\")\n",
        "#         return None\n",
        "\n",
        "\n",
        "# # Store results for comparison\n",
        "# parallel_results = {}\n",
        "\n",
        "# # Run experiments with different worker counts\n",
        "# # Adjust based on your system's CPU cores\n",
        "# worker_counts = [2, 4]  # Add more if you have more cores\n",
        "\n",
        "# print(\"Starting parallel training experiments...\")\n",
        "# print(f\"Configuration: Epochs={EPOCHS}, Batch Size={BATCH_SIZE}\")\n",
        "\n",
        "# for num_workers in worker_counts:\n",
        "#     results = run_parallel_experiment(num_workers, epochs=EPOCHS)\n",
        "#     if results:\n",
        "#         parallel_results[num_workers] = results\n",
        "#         print(f\"\\nCompleted {num_workers}-worker training in {results['total_time']:.2f}s\")\n",
        "\n",
        "# print(\"\\n\" + \"=\"*60)\n",
        "# print(\"All parallel experiments completed!\")\n",
        "# print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "565659bd",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 4: Performance Analysis\n",
        "\n",
        "Now we analyze and compare the performance of serial vs. parallel implementations.\n",
        "\n",
        "### 4.1 Performance Metrics\n",
        "\n",
        "Key metrics for evaluating parallel training:\n",
        "\n",
        "| Metric | Formula | Description |\n",
        "|--------|---------|-------------|\n",
        "| **Speedup** | $S(p) = \\frac{T_{serial}}{T_{parallel}(p)}$ | How much faster with p workers |\n",
        "| **Efficiency** | $E(p) = \\frac{S(p)}{p} = \\frac{T_{serial}}{p \\cdot T_{parallel}(p)}$ | Fraction of ideal speedup achieved |\n",
        "| **Scalability** | How speedup changes with p | Linear = ideal, sublinear = typical |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0cfd60ce",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 14: Performance Comparison and Analysis\n",
        "\n",
        "def compute_performance_metrics(serial_time, parallel_results):\n",
        "    \"\"\"\n",
        "    Compute speedup and efficiency for each parallel configuration.\n",
        "    \n",
        "    Args:\n",
        "        serial_time: Total time for serial training\n",
        "        parallel_results: Dict mapping num_workers -> results\n",
        "        \n",
        "    Returns:\n",
        "        DataFrame with performance metrics\n",
        "    \"\"\"\n",
        "    metrics = []\n",
        "    \n",
        "    # Add serial baseline\n",
        "    metrics.append({\n",
        "        'Workers': 1,\n",
        "        'Total Time (s)': serial_time,\n",
        "        'Avg Epoch Time (s)': serial_time / EPOCHS,\n",
        "        'Speedup': 1.0,\n",
        "        'Efficiency': 1.0,\n",
        "        'Test Accuracy (%)': serial_metrics.test_accuracies[-1]\n",
        "    })\n",
        "    \n",
        "    # Add parallel results\n",
        "    for num_workers, results in sorted(parallel_results.items()):\n",
        "        parallel_time = results['total_time']\n",
        "        speedup = serial_time / parallel_time\n",
        "        efficiency = speedup / num_workers\n",
        "        \n",
        "        metrics.append({\n",
        "            'Workers': num_workers,\n",
        "            'Total Time (s)': parallel_time,\n",
        "            'Avg Epoch Time (s)': parallel_time / EPOCHS,\n",
        "            'Speedup': speedup,\n",
        "            'Efficiency': efficiency,\n",
        "            'Test Accuracy (%)': results['test_accuracies'][-1]\n",
        "        })\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "\n",
        "# Compute metrics\n",
        "if parallel_results:\n",
        "    performance_metrics = compute_performance_metrics(serial_total_time, parallel_results)\n",
        "    \n",
        "    # Display as table\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"PERFORMANCE COMPARISON: SERIAL vs PARALLEL\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"\\n{'Workers':^10} | {'Time (s)':^12} | {'Speedup':^10} | {'Efficiency':^10} | {'Test Acc':^10}\")\n",
        "    print(\"-\"*60)\n",
        "    \n",
        "    for m in performance_metrics:\n",
        "        print(f\"{m['Workers']:^10} | {m['Total Time (s)']:^12.2f} | \"\n",
        "              f\"{m['Speedup']:^10.2f}x | {m['Efficiency']:^10.1%} | \"\n",
        "              f\"{m['Test Accuracy (%)']:^10.2f}%\")\n",
        "    \n",
        "    print(\"-\"*60)\n",
        "    print(\"\\nInterpretation:\")\n",
        "    print(\"- Speedup > 1 means parallel is faster than serial\")\n",
        "    print(\"- Efficiency = 100% means perfect linear scaling (ideal)\")\n",
        "    print(\"- Efficiency < 100% is expected due to communication overhead\")\n",
        "else:\n",
        "    print(\"No parallel results available. Run parallel experiments first.\")\n",
        "    # Create dummy metrics for demonstration\n",
        "    performance_metrics = [\n",
        "        {'Workers': 1, 'Total Time (s)': serial_total_time, 'Speedup': 1.0, \n",
        "         'Efficiency': 1.0, 'Test Accuracy (%)': serial_metrics.test_accuracies[-1]}\n",
        "    ]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fca85fd",
      "metadata": {},
      "source": [
        "### 4.2 Performance Visualization\n",
        "\n",
        "Visualizing speedup, efficiency, and training curves helps understand parallel scaling behavior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5fbdc0f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 15: Performance Visualization\n",
        "\n",
        "def plot_performance_comparison(serial_metrics, parallel_results, performance_metrics):\n",
        "    \"\"\"\n",
        "    Create comprehensive performance visualization plots.\n",
        "    \"\"\"\n",
        "    fig = plt.figure(figsize=(16, 12))\n",
        "    \n",
        "    # Extract data for plotting\n",
        "    workers = [m['Workers'] for m in performance_metrics]\n",
        "    times = [m['Total Time (s)'] for m in performance_metrics]\n",
        "    speedups = [m['Speedup'] for m in performance_metrics]\n",
        "    efficiencies = [m['Efficiency'] * 100 for m in performance_metrics]\n",
        "    \n",
        "    # Plot 1: Training Time vs Workers\n",
        "    ax1 = fig.add_subplot(2, 3, 1)\n",
        "    ax1.bar(range(len(workers)), times, color='steelblue', alpha=0.7)\n",
        "    ax1.set_xticks(range(len(workers)))\n",
        "    ax1.set_xticklabels([str(w) for w in workers])\n",
        "    ax1.set_xlabel('Number of Workers')\n",
        "    ax1.set_ylabel('Total Training Time (s)')\n",
        "    ax1.set_title('Training Time vs Workers')\n",
        "    for i, t in enumerate(times):\n",
        "        ax1.annotate(f'{t:.1f}s', (i, t), ha='center', va='bottom')\n",
        "    ax1.grid(True, alpha=0.3, axis='y')\n",
        "    \n",
        "    # Plot 2: Speedup Curve\n",
        "    ax2 = fig.add_subplot(2, 3, 2)\n",
        "    ax2.plot(workers, speedups, 'bo-', linewidth=2, markersize=10, label='Actual Speedup')\n",
        "    ax2.plot(workers, workers, 'r--', linewidth=2, label='Ideal Linear Speedup')\n",
        "    ax2.set_xlabel('Number of Workers')\n",
        "    ax2.set_ylabel('Speedup')\n",
        "    ax2.set_title('Speedup Curve')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    ax2.set_xlim(0.5, max(workers) + 0.5)\n",
        "    ax2.set_ylim(0, max(workers) + 0.5)\n",
        "    \n",
        "    # Plot 3: Parallel Efficiency\n",
        "    ax3 = fig.add_subplot(2, 3, 3)\n",
        "    colors = ['green' if e >= 70 else 'orange' if e >= 50 else 'red' for e in efficiencies]\n",
        "    ax3.bar(range(len(workers)), efficiencies, color=colors, alpha=0.7)\n",
        "    ax3.axhline(y=100, color='green', linestyle='--', label='Ideal (100%)')\n",
        "    ax3.axhline(y=70, color='orange', linestyle=':', label='Good (70%)')\n",
        "    ax3.set_xticks(range(len(workers)))\n",
        "    ax3.set_xticklabels([str(w) for w in workers])\n",
        "    ax3.set_xlabel('Number of Workers')\n",
        "    ax3.set_ylabel('Efficiency (%)')\n",
        "    ax3.set_title('Parallel Efficiency')\n",
        "    ax3.legend()\n",
        "    ax3.set_ylim(0, 110)\n",
        "    ax3.grid(True, alpha=0.3, axis='y')\n",
        "    \n",
        "    # Plot 4: Loss Curves Comparison\n",
        "    ax4 = fig.add_subplot(2, 3, 4)\n",
        "    epochs_range = range(1, EPOCHS + 1)\n",
        "    ax4.plot(epochs_range, serial_metrics.train_losses, 'b-', linewidth=2, label='Serial')\n",
        "    \n",
        "    for num_workers, results in sorted(parallel_results.items()):\n",
        "        ax4.plot(epochs_range, results['train_losses'], '--', linewidth=2, \n",
        "                label=f'{num_workers} Workers')\n",
        "    \n",
        "    ax4.set_xlabel('Epoch')\n",
        "    ax4.set_ylabel('Training Loss')\n",
        "    ax4.set_title('Loss Convergence: Serial vs Parallel')\n",
        "    ax4.legend()\n",
        "    ax4.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot 5: Test Accuracy Comparison\n",
        "    ax5 = fig.add_subplot(2, 3, 5)\n",
        "    ax5.plot(epochs_range, serial_metrics.test_accuracies, 'b-', linewidth=2, label='Serial')\n",
        "    \n",
        "    for num_workers, results in sorted(parallel_results.items()):\n",
        "        ax5.plot(epochs_range, results['test_accuracies'], '--', linewidth=2,\n",
        "                label=f'{num_workers} Workers')\n",
        "    \n",
        "    ax5.set_xlabel('Epoch')\n",
        "    ax5.set_ylabel('Test Accuracy (%)')\n",
        "    ax5.set_title('Test Accuracy: Serial vs Parallel')\n",
        "    ax5.legend()\n",
        "    ax5.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot 6: Epoch Time Comparison\n",
        "    ax6 = fig.add_subplot(2, 3, 6)\n",
        "    x = np.arange(len(workers))\n",
        "    avg_epoch_times = [m['Total Time (s)'] / EPOCHS for m in performance_metrics]\n",
        "    ax6.bar(x, avg_epoch_times, color='purple', alpha=0.7)\n",
        "    ax6.set_xticks(x)\n",
        "    ax6.set_xticklabels([str(w) for w in workers])\n",
        "    ax6.set_xlabel('Number of Workers')\n",
        "    ax6.set_ylabel('Average Epoch Time (s)')\n",
        "    ax6.set_title('Average Time per Epoch')\n",
        "    for i, t in enumerate(avg_epoch_times):\n",
        "        ax6.annotate(f'{t:.2f}s', (i, t), ha='center', va='bottom')\n",
        "    ax6.grid(True, alpha=0.3, axis='y')\n",
        "    \n",
        "    plt.suptitle('Parallel Training Performance Analysis', fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('performance_analysis.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    print(\"\\nPlot saved to: performance_analysis.png\")\n",
        "\n",
        "\n",
        "# Generate visualization\n",
        "if parallel_results:\n",
        "    plot_performance_comparison(serial_metrics, parallel_results, performance_metrics)\n",
        "else:\n",
        "    print(\"No parallel results available for visualization.\")\n",
        "    print(\"Showing serial results only...\")\n",
        "    \n",
        "    # Plot serial results only\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "    \n",
        "    epochs_range = range(1, EPOCHS + 1)\n",
        "    axes[0].plot(epochs_range, serial_metrics.train_losses, 'b-', label='Train', linewidth=2)\n",
        "    axes[0].plot(epochs_range, serial_metrics.test_losses, 'r-', label='Test', linewidth=2)\n",
        "    axes[0].set_xlabel('Epoch')\n",
        "    axes[0].set_ylabel('Loss')\n",
        "    axes[0].set_title('Serial Training Loss')\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "    \n",
        "    axes[1].plot(epochs_range, serial_metrics.test_accuracies, 'g-', linewidth=2)\n",
        "    axes[1].set_xlabel('Epoch')\n",
        "    axes[1].set_ylabel('Accuracy (%)')\n",
        "    axes[1].set_title('Serial Test Accuracy')\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af2ca6a6",
      "metadata": {},
      "source": [
        "### 4.3 Discussion and Conclusions\n",
        "\n",
        "#### Performance Challenges Encountered\n",
        "\n",
        "1. **Communication Overhead**\n",
        "   - AllReduce operations require synchronization across all workers\n",
        "   - Network/inter-process communication adds latency\n",
        "   - Impact increases with number of workers\n",
        "\n",
        "2. **Synchronization Barriers**  \n",
        "   - All workers must complete their local computation before AllReduce\n",
        "   - Slowest worker determines the pace (straggler effect)\n",
        "   - Load imbalance reduces efficiency\n",
        "\n",
        "3. **Effective Batch Size Scaling**\n",
        "   - With N workers, effective batch size = local_batch × N\n",
        "   - Larger effective batches may require learning rate adjustment\n",
        "   - Linear scaling rule: lr_parallel = lr_serial × N (with warmup)\n",
        "\n",
        "4. **Memory Overhead**\n",
        "   - Each worker maintains a full model copy\n",
        "   - Additional memory for gradient communication buffers\n",
        "\n",
        "#### Trade-offs Analysis\n",
        "\n",
        "| Aspect | Serial | Parallel |\n",
        "|--------|--------|----------|\n",
        "| **Implementation Complexity** | Simple | More complex (process management, synchronization) |\n",
        "| **Scalability** | Limited to single core | Scales with available workers |\n",
        "| **Resource Utilization** | Single CPU/GPU | Multiple CPUs/GPUs |\n",
        "| **Communication Cost** | None | AllReduce overhead |\n",
        "| **Debugging** | Straightforward | More challenging (distributed issues) |\n",
        "\n",
        "#### When to Use Data Parallelism\n",
        "\n",
        "**Good fit:**\n",
        "- Large datasets that benefit from increased throughput\n",
        "- Models that fit in single GPU/CPU memory\n",
        "- Training time is a bottleneck\n",
        "\n",
        "**Less suitable:**\n",
        "- Very small datasets (communication overhead dominates)\n",
        "- Memory-limited scenarios (model parallelism may be better)\n",
        "- Highly sequential models (limited parallelization opportunities)\n",
        "\n",
        "#### Conclusion\n",
        "\n",
        "Data parallelism using PyTorch's DistributedDataParallel provides an effective way to accelerate deep learning training by:\n",
        "\n",
        "1. **Distributing computation** across multiple workers\n",
        "2. **Synchronizing gradients** via efficient AllReduce operations\n",
        "3. **Maintaining model consistency** across all workers\n",
        "\n",
        "The implementation demonstrates sublinear speedup due to communication overhead, which is expected in practice. The trade-off between parallelization benefits and synchronization costs depends on model size, dataset size, and available hardware."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bddd92a5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 16: Summary Statistics and Final Report\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"                    FINAL EXPERIMENT SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\n--- EXPERIMENTAL SETUP ---\")\n",
        "print(f\"Model:           SimpleCNN (~{sum(p.numel() for p in SimpleCNN().parameters()):,} parameters)\")\n",
        "print(f\"Dataset:         CIFAR-10 (50,000 training, 10,000 test images)\")\n",
        "print(f\"Epochs:          {EPOCHS}\")\n",
        "print(f\"Batch Size:      {BATCH_SIZE} (per worker)\")\n",
        "print(f\"Learning Rate:   {LEARNING_RATE}\")\n",
        "print(f\"Optimizer:       SGD with momentum={MOMENTUM}\")\n",
        "print(f\"Device:          {device}\")\n",
        "\n",
        "print(\"\\n--- SERIAL BASELINE ---\")\n",
        "print(f\"Total Time:      {serial_total_time:.2f} seconds\")\n",
        "print(f\"Avg Epoch Time:  {np.mean(serial_metrics.epoch_times):.2f} seconds\")\n",
        "print(f\"Final Test Acc:  {serial_metrics.test_accuracies[-1]:.2f}%\")\n",
        "print(f\"Best Test Acc:   {max(serial_metrics.test_accuracies):.2f}%\")\n",
        "\n",
        "if parallel_results:\n",
        "    print(\"\\n--- PARALLEL RESULTS ---\")\n",
        "    for num_workers, results in sorted(parallel_results.items()):\n",
        "        speedup = serial_total_time / results['total_time']\n",
        "        efficiency = speedup / num_workers * 100\n",
        "        print(f\"\\n{num_workers} Workers:\")\n",
        "        print(f\"  Total Time:    {results['total_time']:.2f} seconds\")\n",
        "        print(f\"  Speedup:       {speedup:.2f}x\")\n",
        "        print(f\"  Efficiency:    {efficiency:.1f}%\")\n",
        "        print(f\"  Final Test Acc: {results['test_accuracies'][-1]:.2f}%\")\n",
        "\n",
        "print(\"\\n--- KEY OBSERVATIONS ---\")\n",
        "print(\"1. Communication overhead reduces parallel efficiency below ideal\")\n",
        "print(\"2. Loss convergence is consistent across serial and parallel runs\")\n",
        "print(\"3. Test accuracy is preserved (correctness verified)\")\n",
        "print(\"4. DDP automates gradient synchronization via Ring-AllReduce\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"                    EXPERIMENT COMPLETE\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c6cb01a",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e08a523",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
